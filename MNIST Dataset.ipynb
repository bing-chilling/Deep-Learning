{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fcf7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6862b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1= nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2= nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1= nn.Dropout2d(0.10)\n",
    "        self.dp2= nn.Dropout2d(0.25)\n",
    "        self.fc1= nn.Linear(4608, 64) #4608= 12*12*32\n",
    "        self.fc2= nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= self.cn1(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.cn2(x)\n",
    "        x= F.relu(x)\n",
    "        x= F.max_pool2d(x,2)\n",
    "        x= self.dp1(x)\n",
    "        x= torch.flatten(x, 1)\n",
    "        x= self.fc1(x)\n",
    "        x= F.relu(x)\n",
    "        x= self.dp2(x)\n",
    "        x= self.fc2(x)\n",
    "        op= F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return op\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89052e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X,y) in enumerate(train_dataloader):\n",
    "        X,y= X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob= model(X)\n",
    "        loss= F.nll_loss(pred_prob, y) #nll-> negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i%10==0:\n",
    "            print('Epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'\n",
    "                    .format(\n",
    "                        epoch,\n",
    "                        b_i*len(X),\n",
    "                        len(train_dataloader.dataset),\n",
    "                        100.*b_i/len(train_dataloader),\n",
    "                        loss.item()\n",
    "                           ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f266750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss=0\n",
    "    success=0\n",
    "    with torch.no_grad():\n",
    "        for X,y in test_dataloader:\n",
    "            X,y= X.to(device), y.to(device)\n",
    "            pred_prob= model(X)\n",
    "            loss+= F.nll_loss(pred_prob, y, reduction='sum').item()\n",
    "            #loss summed across the batch\n",
    "            pred= pred_prob.argmax(dim=1, keepdim=True)\n",
    "            #use argmax to get the most likely prediction\n",
    "            success+= pred.eq(y.view_as(pred)).sum().item()\n",
    "    loss/= len(test_dataloader.dataset)\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            loss, \n",
    "            success, \n",
    "            len(test_dataloader.dataset),\n",
    "            100.*success/len(test_dataloader.dataset)\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ddb1b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader= torch.utils.data.DataLoader(\n",
    "                datasets.MNIST('../data', \n",
    "                            train=True, \n",
    "                            download=True,\n",
    "                            transform= transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1302,),(0.3069,))])),\n",
    "                            #train_X.mean()/256 and train_x.std()/256\n",
    "                            \n",
    "                            batch_size= 32,\n",
    "                            shuffle= True\n",
    "                        \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fdb1c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader= torch.utils.data.DataLoader(\n",
    "                    datasets.MNIST(\n",
    "                        '../data', train=False,\n",
    "                        transform= transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.1302,),(0.3069,))\n",
    "                            ])\n",
    "                        ),\n",
    "                    batch_size=500,\n",
    "                    shuffle= False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "43757d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device= torch.device(\"cpu\")\n",
    "model= ConvNet()\n",
    "optimizer= optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d252434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [0/60000 (0%)]\t training loss: 2.301478\n",
      "Epoch: 1 [320/60000 (1%)]\t training loss: 1.744135\n",
      "Epoch: 1 [640/60000 (1%)]\t training loss: 1.240407\n",
      "Epoch: 1 [960/60000 (2%)]\t training loss: 0.842911\n",
      "Epoch: 1 [1280/60000 (2%)]\t training loss: 0.949035\n",
      "Epoch: 1 [1600/60000 (3%)]\t training loss: 0.558535\n",
      "Epoch: 1 [1920/60000 (3%)]\t training loss: 0.620267\n",
      "Epoch: 1 [2240/60000 (4%)]\t training loss: 0.507514\n",
      "Epoch: 1 [2560/60000 (4%)]\t training loss: 0.467612\n",
      "Epoch: 1 [2880/60000 (5%)]\t training loss: 0.333438\n",
      "Epoch: 1 [3200/60000 (5%)]\t training loss: 0.370361\n",
      "Epoch: 1 [3520/60000 (6%)]\t training loss: 0.361129\n",
      "Epoch: 1 [3840/60000 (6%)]\t training loss: 0.330249\n",
      "Epoch: 1 [4160/60000 (7%)]\t training loss: 0.451925\n",
      "Epoch: 1 [4480/60000 (7%)]\t training loss: 0.376810\n",
      "Epoch: 1 [4800/60000 (8%)]\t training loss: 0.462895\n",
      "Epoch: 1 [5120/60000 (9%)]\t training loss: 0.204568\n",
      "Epoch: 1 [5440/60000 (9%)]\t training loss: 0.309104\n",
      "Epoch: 1 [5760/60000 (10%)]\t training loss: 0.118858\n",
      "Epoch: 1 [6080/60000 (10%)]\t training loss: 0.355564\n",
      "Epoch: 1 [6400/60000 (11%)]\t training loss: 0.250659\n",
      "Epoch: 1 [6720/60000 (11%)]\t training loss: 0.076082\n",
      "Epoch: 1 [7040/60000 (12%)]\t training loss: 0.280036\n",
      "Epoch: 1 [7360/60000 (12%)]\t training loss: 0.093675\n",
      "Epoch: 1 [7680/60000 (13%)]\t training loss: 0.226667\n",
      "Epoch: 1 [8000/60000 (13%)]\t training loss: 0.207803\n",
      "Epoch: 1 [8320/60000 (14%)]\t training loss: 0.353447\n",
      "Epoch: 1 [8640/60000 (14%)]\t training loss: 0.104661\n",
      "Epoch: 1 [8960/60000 (15%)]\t training loss: 0.085475\n",
      "Epoch: 1 [9280/60000 (15%)]\t training loss: 0.279254\n",
      "Epoch: 1 [9600/60000 (16%)]\t training loss: 0.228617\n",
      "Epoch: 1 [9920/60000 (17%)]\t training loss: 0.240107\n",
      "Epoch: 1 [10240/60000 (17%)]\t training loss: 0.257703\n",
      "Epoch: 1 [10560/60000 (18%)]\t training loss: 0.525680\n",
      "Epoch: 1 [10880/60000 (18%)]\t training loss: 0.084281\n",
      "Epoch: 1 [11200/60000 (19%)]\t training loss: 0.172219\n",
      "Epoch: 1 [11520/60000 (19%)]\t training loss: 0.290975\n",
      "Epoch: 1 [11840/60000 (20%)]\t training loss: 0.055079\n",
      "Epoch: 1 [12160/60000 (20%)]\t training loss: 0.148387\n",
      "Epoch: 1 [12480/60000 (21%)]\t training loss: 0.044548\n",
      "Epoch: 1 [12800/60000 (21%)]\t training loss: 0.127944\n",
      "Epoch: 1 [13120/60000 (22%)]\t training loss: 0.123009\n",
      "Epoch: 1 [13440/60000 (22%)]\t training loss: 0.093149\n",
      "Epoch: 1 [13760/60000 (23%)]\t training loss: 0.050868\n",
      "Epoch: 1 [14080/60000 (23%)]\t training loss: 0.039197\n",
      "Epoch: 1 [14400/60000 (24%)]\t training loss: 0.111042\n",
      "Epoch: 1 [14720/60000 (25%)]\t training loss: 0.156271\n",
      "Epoch: 1 [15040/60000 (25%)]\t training loss: 0.112623\n",
      "Epoch: 1 [15360/60000 (26%)]\t training loss: 0.201284\n",
      "Epoch: 1 [15680/60000 (26%)]\t training loss: 0.233557\n",
      "Epoch: 1 [16000/60000 (27%)]\t training loss: 0.178375\n",
      "Epoch: 1 [16320/60000 (27%)]\t training loss: 0.061497\n",
      "Epoch: 1 [16640/60000 (28%)]\t training loss: 0.047219\n",
      "Epoch: 1 [16960/60000 (28%)]\t training loss: 0.103287\n",
      "Epoch: 1 [17280/60000 (29%)]\t training loss: 0.045609\n",
      "Epoch: 1 [17600/60000 (29%)]\t training loss: 0.073478\n",
      "Epoch: 1 [17920/60000 (30%)]\t training loss: 0.044403\n",
      "Epoch: 1 [18240/60000 (30%)]\t training loss: 0.020452\n",
      "Epoch: 1 [18560/60000 (31%)]\t training loss: 0.096137\n",
      "Epoch: 1 [18880/60000 (31%)]\t training loss: 0.306130\n",
      "Epoch: 1 [19200/60000 (32%)]\t training loss: 0.126292\n",
      "Epoch: 1 [19520/60000 (33%)]\t training loss: 0.098491\n",
      "Epoch: 1 [19840/60000 (33%)]\t training loss: 0.097359\n",
      "Epoch: 1 [20160/60000 (34%)]\t training loss: 0.072178\n",
      "Epoch: 1 [20480/60000 (34%)]\t training loss: 0.076306\n",
      "Epoch: 1 [20800/60000 (35%)]\t training loss: 0.094879\n",
      "Epoch: 1 [21120/60000 (35%)]\t training loss: 0.330756\n",
      "Epoch: 1 [21440/60000 (36%)]\t training loss: 0.070116\n",
      "Epoch: 1 [21760/60000 (36%)]\t training loss: 0.144586\n",
      "Epoch: 1 [22080/60000 (37%)]\t training loss: 0.064467\n",
      "Epoch: 1 [22400/60000 (37%)]\t training loss: 0.213577\n",
      "Epoch: 1 [22720/60000 (38%)]\t training loss: 0.235473\n",
      "Epoch: 1 [23040/60000 (38%)]\t training loss: 0.034639\n",
      "Epoch: 1 [23360/60000 (39%)]\t training loss: 0.199369\n",
      "Epoch: 1 [23680/60000 (39%)]\t training loss: 0.334644\n",
      "Epoch: 1 [24000/60000 (40%)]\t training loss: 0.114927\n",
      "Epoch: 1 [24320/60000 (41%)]\t training loss: 0.067071\n",
      "Epoch: 1 [24640/60000 (41%)]\t training loss: 0.204310\n",
      "Epoch: 1 [24960/60000 (42%)]\t training loss: 0.067791\n",
      "Epoch: 1 [25280/60000 (42%)]\t training loss: 0.041735\n",
      "Epoch: 1 [25600/60000 (43%)]\t training loss: 0.302166\n",
      "Epoch: 1 [25920/60000 (43%)]\t training loss: 0.604861\n",
      "Epoch: 1 [26240/60000 (44%)]\t training loss: 0.037603\n",
      "Epoch: 1 [26560/60000 (44%)]\t training loss: 0.041440\n",
      "Epoch: 1 [26880/60000 (45%)]\t training loss: 0.204493\n",
      "Epoch: 1 [27200/60000 (45%)]\t training loss: 0.044343\n",
      "Epoch: 1 [27520/60000 (46%)]\t training loss: 0.013592\n",
      "Epoch: 1 [27840/60000 (46%)]\t training loss: 0.134630\n",
      "Epoch: 1 [28160/60000 (47%)]\t training loss: 0.006786\n",
      "Epoch: 1 [28480/60000 (47%)]\t training loss: 0.193019\n",
      "Epoch: 1 [28800/60000 (48%)]\t training loss: 0.118102\n",
      "Epoch: 1 [29120/60000 (49%)]\t training loss: 0.417700\n",
      "Epoch: 1 [29440/60000 (49%)]\t training loss: 0.009899\n",
      "Epoch: 1 [29760/60000 (50%)]\t training loss: 0.085752\n",
      "Epoch: 1 [30080/60000 (50%)]\t training loss: 0.371418\n",
      "Epoch: 1 [30400/60000 (51%)]\t training loss: 0.035111\n",
      "Epoch: 1 [30720/60000 (51%)]\t training loss: 0.039869\n",
      "Epoch: 1 [31040/60000 (52%)]\t training loss: 0.093572\n",
      "Epoch: 1 [31360/60000 (52%)]\t training loss: 0.045772\n",
      "Epoch: 1 [31680/60000 (53%)]\t training loss: 0.024503\n",
      "Epoch: 1 [32000/60000 (53%)]\t training loss: 0.002575\n",
      "Epoch: 1 [32320/60000 (54%)]\t training loss: 0.059168\n",
      "Epoch: 1 [32640/60000 (54%)]\t training loss: 0.195339\n",
      "Epoch: 1 [32960/60000 (55%)]\t training loss: 0.004655\n",
      "Epoch: 1 [33280/60000 (55%)]\t training loss: 0.440067\n",
      "Epoch: 1 [33600/60000 (56%)]\t training loss: 0.078800\n",
      "Epoch: 1 [33920/60000 (57%)]\t training loss: 0.054055\n",
      "Epoch: 1 [34240/60000 (57%)]\t training loss: 0.035785\n",
      "Epoch: 1 [34560/60000 (58%)]\t training loss: 0.009613\n",
      "Epoch: 1 [34880/60000 (58%)]\t training loss: 0.115246\n",
      "Epoch: 1 [35200/60000 (59%)]\t training loss: 0.140150\n",
      "Epoch: 1 [35520/60000 (59%)]\t training loss: 0.080071\n",
      "Epoch: 1 [35840/60000 (60%)]\t training loss: 0.037469\n",
      "Epoch: 1 [36160/60000 (60%)]\t training loss: 0.005697\n",
      "Epoch: 1 [36480/60000 (61%)]\t training loss: 0.080812\n",
      "Epoch: 1 [36800/60000 (61%)]\t training loss: 0.131422\n",
      "Epoch: 1 [37120/60000 (62%)]\t training loss: 0.231469\n",
      "Epoch: 1 [37440/60000 (62%)]\t training loss: 0.139390\n",
      "Epoch: 1 [37760/60000 (63%)]\t training loss: 0.016941\n",
      "Epoch: 1 [38080/60000 (63%)]\t training loss: 0.105850\n",
      "Epoch: 1 [38400/60000 (64%)]\t training loss: 0.154422\n",
      "Epoch: 1 [38720/60000 (65%)]\t training loss: 0.240857\n",
      "Epoch: 1 [39040/60000 (65%)]\t training loss: 0.012485\n",
      "Epoch: 1 [39360/60000 (66%)]\t training loss: 0.296830\n",
      "Epoch: 1 [39680/60000 (66%)]\t training loss: 0.038216\n",
      "Epoch: 1 [40000/60000 (67%)]\t training loss: 0.076197\n",
      "Epoch: 1 [40320/60000 (67%)]\t training loss: 0.059942\n",
      "Epoch: 1 [40640/60000 (68%)]\t training loss: 0.147952\n",
      "Epoch: 1 [40960/60000 (68%)]\t training loss: 0.162856\n",
      "Epoch: 1 [41280/60000 (69%)]\t training loss: 0.272731\n",
      "Epoch: 1 [41600/60000 (69%)]\t training loss: 0.172462\n",
      "Epoch: 1 [41920/60000 (70%)]\t training loss: 0.028753\n",
      "Epoch: 1 [42240/60000 (70%)]\t training loss: 0.013197\n",
      "Epoch: 1 [42560/60000 (71%)]\t training loss: 0.025361\n",
      "Epoch: 1 [42880/60000 (71%)]\t training loss: 0.133646\n",
      "Epoch: 1 [43200/60000 (72%)]\t training loss: 0.040025\n",
      "Epoch: 1 [43520/60000 (73%)]\t training loss: 0.165667\n",
      "Epoch: 1 [43840/60000 (73%)]\t training loss: 0.104438\n",
      "Epoch: 1 [44160/60000 (74%)]\t training loss: 0.154451\n",
      "Epoch: 1 [44480/60000 (74%)]\t training loss: 0.057280\n",
      "Epoch: 1 [44800/60000 (75%)]\t training loss: 0.286511\n",
      "Epoch: 1 [45120/60000 (75%)]\t training loss: 0.099609\n",
      "Epoch: 1 [45440/60000 (76%)]\t training loss: 0.152643\n",
      "Epoch: 1 [45760/60000 (76%)]\t training loss: 0.013211\n",
      "Epoch: 1 [46080/60000 (77%)]\t training loss: 0.462148\n",
      "Epoch: 1 [46400/60000 (77%)]\t training loss: 0.079826\n",
      "Epoch: 1 [46720/60000 (78%)]\t training loss: 0.189882\n",
      "Epoch: 1 [47040/60000 (78%)]\t training loss: 0.108566\n",
      "Epoch: 1 [47360/60000 (79%)]\t training loss: 0.067054\n",
      "Epoch: 1 [47680/60000 (79%)]\t training loss: 0.101602\n",
      "Epoch: 1 [48000/60000 (80%)]\t training loss: 0.138708\n",
      "Epoch: 1 [48320/60000 (81%)]\t training loss: 0.131902\n",
      "Epoch: 1 [48640/60000 (81%)]\t training loss: 0.017348\n",
      "Epoch: 1 [48960/60000 (82%)]\t training loss: 0.010339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 [49280/60000 (82%)]\t training loss: 0.229838\n",
      "Epoch: 1 [49600/60000 (83%)]\t training loss: 0.067799\n",
      "Epoch: 1 [49920/60000 (83%)]\t training loss: 0.010049\n",
      "Epoch: 1 [50240/60000 (84%)]\t training loss: 0.121754\n",
      "Epoch: 1 [50560/60000 (84%)]\t training loss: 0.005505\n",
      "Epoch: 1 [50880/60000 (85%)]\t training loss: 0.007708\n",
      "Epoch: 1 [51200/60000 (85%)]\t training loss: 0.251524\n",
      "Epoch: 1 [51520/60000 (86%)]\t training loss: 0.007059\n",
      "Epoch: 1 [51840/60000 (86%)]\t training loss: 0.020428\n",
      "Epoch: 1 [52160/60000 (87%)]\t training loss: 0.010024\n",
      "Epoch: 1 [52480/60000 (87%)]\t training loss: 0.027585\n",
      "Epoch: 1 [52800/60000 (88%)]\t training loss: 0.058778\n",
      "Epoch: 1 [53120/60000 (89%)]\t training loss: 0.043344\n",
      "Epoch: 1 [53440/60000 (89%)]\t training loss: 0.086786\n",
      "Epoch: 1 [53760/60000 (90%)]\t training loss: 0.084133\n",
      "Epoch: 1 [54080/60000 (90%)]\t training loss: 0.039657\n",
      "Epoch: 1 [54400/60000 (91%)]\t training loss: 0.104869\n",
      "Epoch: 1 [54720/60000 (91%)]\t training loss: 0.035542\n",
      "Epoch: 1 [55040/60000 (92%)]\t training loss: 0.003312\n",
      "Epoch: 1 [55360/60000 (92%)]\t training loss: 0.104805\n",
      "Epoch: 1 [55680/60000 (93%)]\t training loss: 0.022431\n",
      "Epoch: 1 [56000/60000 (93%)]\t training loss: 0.108307\n",
      "Epoch: 1 [56320/60000 (94%)]\t training loss: 0.028256\n",
      "Epoch: 1 [56640/60000 (94%)]\t training loss: 0.078716\n",
      "Epoch: 1 [56960/60000 (95%)]\t training loss: 0.042624\n",
      "Epoch: 1 [57280/60000 (95%)]\t training loss: 0.090352\n",
      "Epoch: 1 [57600/60000 (96%)]\t training loss: 0.035861\n",
      "Epoch: 1 [57920/60000 (97%)]\t training loss: 0.033717\n",
      "Epoch: 1 [58240/60000 (97%)]\t training loss: 0.038008\n",
      "Epoch: 1 [58560/60000 (98%)]\t training loss: 0.035116\n",
      "Epoch: 1 [58880/60000 (98%)]\t training loss: 0.047523\n",
      "Epoch: 1 [59200/60000 (99%)]\t training loss: 0.004589\n",
      "Epoch: 1 [59520/60000 (99%)]\t training loss: 0.022190\n",
      "Epoch: 1 [59840/60000 (100%)]\t training loss: 0.039899\n",
      "\n",
      "Test dataset: Overall Loss: 0.0490, Overall Accuracy: 9829/10000 (98%)\n",
      "\n",
      "Epoch: 2 [0/60000 (0%)]\t training loss: 0.063192\n",
      "Epoch: 2 [320/60000 (1%)]\t training loss: 0.035715\n",
      "Epoch: 2 [640/60000 (1%)]\t training loss: 0.157612\n",
      "Epoch: 2 [960/60000 (2%)]\t training loss: 0.104676\n",
      "Epoch: 2 [1280/60000 (2%)]\t training loss: 0.043009\n",
      "Epoch: 2 [1600/60000 (3%)]\t training loss: 0.010249\n",
      "Epoch: 2 [1920/60000 (3%)]\t training loss: 0.027563\n",
      "Epoch: 2 [2240/60000 (4%)]\t training loss: 0.002874\n",
      "Epoch: 2 [2560/60000 (4%)]\t training loss: 0.005963\n",
      "Epoch: 2 [2880/60000 (5%)]\t training loss: 0.047946\n",
      "Epoch: 2 [3200/60000 (5%)]\t training loss: 0.008890\n",
      "Epoch: 2 [3520/60000 (6%)]\t training loss: 0.184991\n",
      "Epoch: 2 [3840/60000 (6%)]\t training loss: 0.036823\n",
      "Epoch: 2 [4160/60000 (7%)]\t training loss: 0.197985\n",
      "Epoch: 2 [4480/60000 (7%)]\t training loss: 0.000780\n",
      "Epoch: 2 [4800/60000 (8%)]\t training loss: 0.044045\n",
      "Epoch: 2 [5120/60000 (9%)]\t training loss: 0.189826\n",
      "Epoch: 2 [5440/60000 (9%)]\t training loss: 0.194079\n",
      "Epoch: 2 [5760/60000 (10%)]\t training loss: 0.006595\n",
      "Epoch: 2 [6080/60000 (10%)]\t training loss: 0.188140\n",
      "Epoch: 2 [6400/60000 (11%)]\t training loss: 0.063899\n",
      "Epoch: 2 [6720/60000 (11%)]\t training loss: 0.137398\n",
      "Epoch: 2 [7040/60000 (12%)]\t training loss: 0.085219\n",
      "Epoch: 2 [7360/60000 (12%)]\t training loss: 0.033088\n",
      "Epoch: 2 [7680/60000 (13%)]\t training loss: 0.104421\n",
      "Epoch: 2 [8000/60000 (13%)]\t training loss: 0.041275\n",
      "Epoch: 2 [8320/60000 (14%)]\t training loss: 0.032096\n",
      "Epoch: 2 [8640/60000 (14%)]\t training loss: 0.082924\n",
      "Epoch: 2 [8960/60000 (15%)]\t training loss: 0.481236\n",
      "Epoch: 2 [9280/60000 (15%)]\t training loss: 0.078906\n",
      "Epoch: 2 [9600/60000 (16%)]\t training loss: 0.036604\n",
      "Epoch: 2 [9920/60000 (17%)]\t training loss: 0.016206\n",
      "Epoch: 2 [10240/60000 (17%)]\t training loss: 0.006723\n",
      "Epoch: 2 [10560/60000 (18%)]\t training loss: 0.084231\n",
      "Epoch: 2 [10880/60000 (18%)]\t training loss: 0.058738\n",
      "Epoch: 2 [11200/60000 (19%)]\t training loss: 0.033219\n",
      "Epoch: 2 [11520/60000 (19%)]\t training loss: 0.038849\n",
      "Epoch: 2 [11840/60000 (20%)]\t training loss: 0.018835\n",
      "Epoch: 2 [12160/60000 (20%)]\t training loss: 0.010459\n",
      "Epoch: 2 [12480/60000 (21%)]\t training loss: 0.051129\n",
      "Epoch: 2 [12800/60000 (21%)]\t training loss: 0.053610\n",
      "Epoch: 2 [13120/60000 (22%)]\t training loss: 0.164905\n",
      "Epoch: 2 [13440/60000 (22%)]\t training loss: 0.026690\n",
      "Epoch: 2 [13760/60000 (23%)]\t training loss: 0.007587\n",
      "Epoch: 2 [14080/60000 (23%)]\t training loss: 0.061856\n",
      "Epoch: 2 [14400/60000 (24%)]\t training loss: 0.022620\n",
      "Epoch: 2 [14720/60000 (25%)]\t training loss: 0.037123\n",
      "Epoch: 2 [15040/60000 (25%)]\t training loss: 0.023474\n",
      "Epoch: 2 [15360/60000 (26%)]\t training loss: 0.212025\n",
      "Epoch: 2 [15680/60000 (26%)]\t training loss: 0.015131\n",
      "Epoch: 2 [16000/60000 (27%)]\t training loss: 0.010486\n",
      "Epoch: 2 [16320/60000 (27%)]\t training loss: 0.026305\n",
      "Epoch: 2 [16640/60000 (28%)]\t training loss: 0.041961\n",
      "Epoch: 2 [16960/60000 (28%)]\t training loss: 0.015478\n",
      "Epoch: 2 [17280/60000 (29%)]\t training loss: 0.055210\n",
      "Epoch: 2 [17600/60000 (29%)]\t training loss: 0.131033\n",
      "Epoch: 2 [17920/60000 (30%)]\t training loss: 0.000790\n",
      "Epoch: 2 [18240/60000 (30%)]\t training loss: 0.056192\n",
      "Epoch: 2 [18560/60000 (31%)]\t training loss: 0.017300\n",
      "Epoch: 2 [18880/60000 (31%)]\t training loss: 0.021709\n",
      "Epoch: 2 [19200/60000 (32%)]\t training loss: 0.015322\n",
      "Epoch: 2 [19520/60000 (33%)]\t training loss: 0.011187\n",
      "Epoch: 2 [19840/60000 (33%)]\t training loss: 0.124674\n",
      "Epoch: 2 [20160/60000 (34%)]\t training loss: 0.008614\n",
      "Epoch: 2 [20480/60000 (34%)]\t training loss: 0.008545\n",
      "Epoch: 2 [20800/60000 (35%)]\t training loss: 0.013849\n",
      "Epoch: 2 [21120/60000 (35%)]\t training loss: 0.092696\n",
      "Epoch: 2 [21440/60000 (36%)]\t training loss: 0.039775\n",
      "Epoch: 2 [21760/60000 (36%)]\t training loss: 0.027067\n",
      "Epoch: 2 [22080/60000 (37%)]\t training loss: 0.067152\n",
      "Epoch: 2 [22400/60000 (37%)]\t training loss: 0.001642\n",
      "Epoch: 2 [22720/60000 (38%)]\t training loss: 0.003181\n",
      "Epoch: 2 [23040/60000 (38%)]\t training loss: 0.012829\n",
      "Epoch: 2 [23360/60000 (39%)]\t training loss: 0.002656\n",
      "Epoch: 2 [23680/60000 (39%)]\t training loss: 0.004579\n",
      "Epoch: 2 [24000/60000 (40%)]\t training loss: 0.065491\n",
      "Epoch: 2 [24320/60000 (41%)]\t training loss: 0.001968\n",
      "Epoch: 2 [24640/60000 (41%)]\t training loss: 0.045747\n",
      "Epoch: 2 [24960/60000 (42%)]\t training loss: 0.045544\n",
      "Epoch: 2 [25280/60000 (42%)]\t training loss: 0.013385\n",
      "Epoch: 2 [25600/60000 (43%)]\t training loss: 0.019852\n",
      "Epoch: 2 [25920/60000 (43%)]\t training loss: 0.001449\n",
      "Epoch: 2 [26240/60000 (44%)]\t training loss: 0.047864\n",
      "Epoch: 2 [26560/60000 (44%)]\t training loss: 0.025001\n",
      "Epoch: 2 [26880/60000 (45%)]\t training loss: 0.063058\n",
      "Epoch: 2 [27200/60000 (45%)]\t training loss: 0.006972\n",
      "Epoch: 2 [27520/60000 (46%)]\t training loss: 0.014942\n",
      "Epoch: 2 [27840/60000 (46%)]\t training loss: 0.065049\n",
      "Epoch: 2 [28160/60000 (47%)]\t training loss: 0.087702\n",
      "Epoch: 2 [28480/60000 (47%)]\t training loss: 0.055587\n",
      "Epoch: 2 [28800/60000 (48%)]\t training loss: 0.035279\n",
      "Epoch: 2 [29120/60000 (49%)]\t training loss: 0.043638\n",
      "Epoch: 2 [29440/60000 (49%)]\t training loss: 0.128099\n",
      "Epoch: 2 [29760/60000 (50%)]\t training loss: 0.020406\n",
      "Epoch: 2 [30080/60000 (50%)]\t training loss: 0.002630\n",
      "Epoch: 2 [30400/60000 (51%)]\t training loss: 0.040567\n",
      "Epoch: 2 [30720/60000 (51%)]\t training loss: 0.366096\n",
      "Epoch: 2 [31040/60000 (52%)]\t training loss: 0.005568\n",
      "Epoch: 2 [31360/60000 (52%)]\t training loss: 0.016480\n",
      "Epoch: 2 [31680/60000 (53%)]\t training loss: 0.051439\n",
      "Epoch: 2 [32000/60000 (53%)]\t training loss: 0.181614\n",
      "Epoch: 2 [32320/60000 (54%)]\t training loss: 0.025319\n",
      "Epoch: 2 [32640/60000 (54%)]\t training loss: 0.096570\n",
      "Epoch: 2 [32960/60000 (55%)]\t training loss: 0.010488\n",
      "Epoch: 2 [33280/60000 (55%)]\t training loss: 0.017428\n",
      "Epoch: 2 [33600/60000 (56%)]\t training loss: 0.393274\n",
      "Epoch: 2 [33920/60000 (57%)]\t training loss: 0.048815\n",
      "Epoch: 2 [34240/60000 (57%)]\t training loss: 0.050560\n",
      "Epoch: 2 [34560/60000 (58%)]\t training loss: 0.008977\n",
      "Epoch: 2 [34880/60000 (58%)]\t training loss: 0.008384\n",
      "Epoch: 2 [35200/60000 (59%)]\t training loss: 0.010363\n",
      "Epoch: 2 [35520/60000 (59%)]\t training loss: 0.012169\n",
      "Epoch: 2 [35840/60000 (60%)]\t training loss: 0.007284\n",
      "Epoch: 2 [36160/60000 (60%)]\t training loss: 0.085685\n",
      "Epoch: 2 [36480/60000 (61%)]\t training loss: 0.068615\n",
      "Epoch: 2 [36800/60000 (61%)]\t training loss: 0.027245\n",
      "Epoch: 2 [37120/60000 (62%)]\t training loss: 0.298310\n",
      "Epoch: 2 [37440/60000 (62%)]\t training loss: 0.116406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 [37760/60000 (63%)]\t training loss: 0.153400\n",
      "Epoch: 2 [38080/60000 (63%)]\t training loss: 0.062601\n",
      "Epoch: 2 [38400/60000 (64%)]\t training loss: 0.119492\n",
      "Epoch: 2 [38720/60000 (65%)]\t training loss: 0.016463\n",
      "Epoch: 2 [39040/60000 (65%)]\t training loss: 0.008085\n",
      "Epoch: 2 [39360/60000 (66%)]\t training loss: 0.027865\n",
      "Epoch: 2 [39680/60000 (66%)]\t training loss: 0.001634\n",
      "Epoch: 2 [40000/60000 (67%)]\t training loss: 0.160155\n",
      "Epoch: 2 [40320/60000 (67%)]\t training loss: 0.040541\n",
      "Epoch: 2 [40640/60000 (68%)]\t training loss: 0.031036\n",
      "Epoch: 2 [40960/60000 (68%)]\t training loss: 0.002685\n",
      "Epoch: 2 [41280/60000 (69%)]\t training loss: 0.022982\n",
      "Epoch: 2 [41600/60000 (69%)]\t training loss: 0.021805\n",
      "Epoch: 2 [41920/60000 (70%)]\t training loss: 0.002257\n",
      "Epoch: 2 [42240/60000 (70%)]\t training loss: 0.000630\n",
      "Epoch: 2 [42560/60000 (71%)]\t training loss: 0.014772\n",
      "Epoch: 2 [42880/60000 (71%)]\t training loss: 0.029629\n",
      "Epoch: 2 [43200/60000 (72%)]\t training loss: 0.028959\n",
      "Epoch: 2 [43520/60000 (73%)]\t training loss: 0.082931\n",
      "Epoch: 2 [43840/60000 (73%)]\t training loss: 0.005638\n",
      "Epoch: 2 [44160/60000 (74%)]\t training loss: 0.119555\n",
      "Epoch: 2 [44480/60000 (74%)]\t training loss: 0.011801\n",
      "Epoch: 2 [44800/60000 (75%)]\t training loss: 0.015940\n",
      "Epoch: 2 [45120/60000 (75%)]\t training loss: 0.007462\n",
      "Epoch: 2 [45440/60000 (76%)]\t training loss: 0.106445\n",
      "Epoch: 2 [45760/60000 (76%)]\t training loss: 0.116790\n",
      "Epoch: 2 [46080/60000 (77%)]\t training loss: 0.193902\n",
      "Epoch: 2 [46400/60000 (77%)]\t training loss: 0.257850\n",
      "Epoch: 2 [46720/60000 (78%)]\t training loss: 0.040314\n",
      "Epoch: 2 [47040/60000 (78%)]\t training loss: 0.003784\n",
      "Epoch: 2 [47360/60000 (79%)]\t training loss: 0.010130\n",
      "Epoch: 2 [47680/60000 (79%)]\t training loss: 0.100068\n",
      "Epoch: 2 [48000/60000 (80%)]\t training loss: 0.253857\n",
      "Epoch: 2 [48320/60000 (81%)]\t training loss: 0.019502\n",
      "Epoch: 2 [48640/60000 (81%)]\t training loss: 0.008677\n",
      "Epoch: 2 [48960/60000 (82%)]\t training loss: 0.078126\n",
      "Epoch: 2 [49280/60000 (82%)]\t training loss: 0.051812\n",
      "Epoch: 2 [49600/60000 (83%)]\t training loss: 0.078486\n",
      "Epoch: 2 [49920/60000 (83%)]\t training loss: 0.012587\n",
      "Epoch: 2 [50240/60000 (84%)]\t training loss: 0.098009\n",
      "Epoch: 2 [50560/60000 (84%)]\t training loss: 0.009857\n",
      "Epoch: 2 [50880/60000 (85%)]\t training loss: 0.381076\n",
      "Epoch: 2 [51200/60000 (85%)]\t training loss: 0.002559\n",
      "Epoch: 2 [51520/60000 (86%)]\t training loss: 0.010299\n",
      "Epoch: 2 [51840/60000 (86%)]\t training loss: 0.549523\n",
      "Epoch: 2 [52160/60000 (87%)]\t training loss: 0.025561\n",
      "Epoch: 2 [52480/60000 (87%)]\t training loss: 0.117823\n",
      "Epoch: 2 [52800/60000 (88%)]\t training loss: 0.306408\n",
      "Epoch: 2 [53120/60000 (89%)]\t training loss: 0.019859\n",
      "Epoch: 2 [53440/60000 (89%)]\t training loss: 0.071886\n",
      "Epoch: 2 [53760/60000 (90%)]\t training loss: 0.053412\n",
      "Epoch: 2 [54080/60000 (90%)]\t training loss: 0.009835\n",
      "Epoch: 2 [54400/60000 (91%)]\t training loss: 0.102760\n",
      "Epoch: 2 [54720/60000 (91%)]\t training loss: 0.462316\n",
      "Epoch: 2 [55040/60000 (92%)]\t training loss: 0.014635\n",
      "Epoch: 2 [55360/60000 (92%)]\t training loss: 0.008721\n",
      "Epoch: 2 [55680/60000 (93%)]\t training loss: 0.155922\n",
      "Epoch: 2 [56000/60000 (93%)]\t training loss: 0.011346\n",
      "Epoch: 2 [56320/60000 (94%)]\t training loss: 0.195575\n",
      "Epoch: 2 [56640/60000 (94%)]\t training loss: 0.439394\n",
      "Epoch: 2 [56960/60000 (95%)]\t training loss: 0.006975\n",
      "Epoch: 2 [57280/60000 (95%)]\t training loss: 0.019518\n",
      "Epoch: 2 [57600/60000 (96%)]\t training loss: 0.123630\n",
      "Epoch: 2 [57920/60000 (97%)]\t training loss: 0.017330\n",
      "Epoch: 2 [58240/60000 (97%)]\t training loss: 0.084489\n",
      "Epoch: 2 [58560/60000 (98%)]\t training loss: 0.013619\n",
      "Epoch: 2 [58880/60000 (98%)]\t training loss: 0.007041\n",
      "Epoch: 2 [59200/60000 (99%)]\t training loss: 0.172996\n",
      "Epoch: 2 [59520/60000 (99%)]\t training loss: 0.112990\n",
      "Epoch: 2 [59840/60000 (100%)]\t training loss: 0.008173\n",
      "\n",
      "Test dataset: Overall Loss: 0.0435, Overall Accuracy: 9843/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c715cf",
   "metadata": {},
   "source": [
    "### Run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f78e759e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples= enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets)= next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "947df05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is: 7\n",
      "Ground truth is: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is: {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is: {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a726a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
